{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: Email Spam Naive Bayes\n",
    "\n",
    "## Overview/Task\n",
    "\n",
    "The goal of this programming assignment is to build a naive bayes classifier from scratch that can determine whether email text should be labled spam or not spam based on its contents\n",
    "\n",
    "## Review\n",
    "\n",
    "Remeber that a naive bayes classifier realizes the following probability:\n",
    "\n",
    "$$P(Y|X_1,X_2,...,X_n) \\propto P(Y)*P(Y|X_1)*P(Y|X_2)*...*P(Y|X_n)$$\n",
    "\n",
    "Where $Y$ is a binary class {0,1}\n",
    "\n",
    "Where $X_i$ is a feature of the input\n",
    "\n",
    "The classifier will decide what class each input belongs to based on highest probability from the equation above\n",
    "\n",
    "## Reminders\n",
    "\n",
    "Please remember that the classifier must be written from scratch; do NOT use any libraries that implement the classifier for you, such as but not limited to sklearn.\n",
    "\n",
    "You CAN, however, use SKlearn to split up the dataset between testing and training.\n",
    "\n",
    "Feel free to look up any tasks you are not familiar with, e.g. the function call to read a csv\n",
    "\n",
    "## Task list/Recommended Order\n",
    "\n",
    "In order to provide some guidance, I am giving the following order/checklist to solve this task:\n",
    "<ol>\n",
    "  <li>Compute the \"prior\": P(Y) for Y = 0 and Y = 1</li>\n",
    "  <li>Compute the \"likelihood\": $P(Y|X_n)$</li>\n",
    "  <li>Write code that uses the two items above to make a decision on whether or not an email is spam or ham (aka not spam)</li>\n",
    "  <li>Write code to evaluate your model. Test model on training data to debug </li>\n",
    "  <li>Test model on testing data to debug </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(df):\n",
    "    ham_prior = 0\n",
    "    spam_prior =  0\n",
    "    '''YOUR CODE HERE'''\n",
    "    ham_count = len(df[df['label'] == 'ham'])\n",
    "    spam_count = len(df[df['label'] == 'spam'])\n",
    "    total_count = len(df)\n",
    "    ham_prior = ham_count / total_count\n",
    "    spam_prior = spam_count / total_count\n",
    "    \n",
    "    return ham_prior, spam_prior\n",
    "    '''END'''\n",
    "def prior(df):\n",
    "    ham_prior = 0\n",
    "    spam_prior = 0\n",
    "    '''YOUR CODE HERE'''\n",
    "    ham_count = len(df[df['label'] == 'ham']) \n",
    "    spam_count = len(df[df['label'] == 'spam']) \n",
    "    total_count = len(df)\n",
    "    ham_prior = ham_count / total_count\n",
    "    spam_prior = ham_count / total_count\n",
    "    \n",
    "    return ham_prior, spam_prior\n",
    "    '''END'''\n",
    "\n",
    "\n",
    "def likelihood(df):\n",
    "    ham_like_dict = {}\n",
    "    spam_like_dict = {}\n",
    "    '''YOUR CODE HERE'''\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    for index, row in df[df['label'] == 'ham'].iterrows():\n",
    "        words = set([i.strip(\"/.,:?!'\\\"\") for i in row['text'].split() if i not in punctuation_set]) \n",
    "        ham_word_counts = {}\n",
    "        for word in words:\n",
    "            if word in ham_word_counts: \n",
    "                ham_word_counts[word] += 1\n",
    "            else:\n",
    "                ham_word_counts[word] = 1\n",
    "        for word, count in ham_word_counts.items(): \n",
    "            if word in ham_like_dict:\n",
    "                ham_like_dict[word] += count \n",
    "            else:\n",
    "                ham_like_dict[word] = count\n",
    "    for index, row in df[df['label'] == 'spam'].iterrows():\n",
    "        words = set([i.strip(\"/.,:?!'\\\"\") for i in row['text'].split() if i not in punctuation_set]) \n",
    "        spam_word_counts = {}\n",
    "        for word in words:\n",
    "            if word in spam_word_counts: \n",
    "                spam_word_counts[word] += 1\n",
    "            else:\n",
    "                spam_word_counts[word] = 1\n",
    "        for word, count in spam_word_counts.items(): \n",
    "            if word in spam_like_dict:\n",
    "                spam_like_dict[word] += count \n",
    "            else:\n",
    "                spam_like_dict[word] = count\n",
    "    return ham_like_dict, spam_like_dict    \n",
    "    '''END'''\n",
    "\n",
    "\n",
    "def predict(ham_prior, spam_prior, ham_like_dict, spam_like_dict, text):\n",
    "    '''\n",
    "    prediction function that uses prior and likelihood structure to compute proportional posterior for a single line of text\n",
    "    '''\n",
    "    '''YOUR CODE HERE'''\n",
    "    #ham_spam_decision = 1 if classified as spam, 0 if classified as normal/ham\n",
    "    ham_spam_decision = None\n",
    "    #ham_posterior = posterior probability that the email is normal/ham\n",
    "    ham_posterior = ham_prior\n",
    "    #spam_posterior = posterior probability that the email is spam\n",
    "    spam_posterior = spam_prior\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word in ham_like_dict:\n",
    "            ham_posterior *= ham_like_dict[word]\n",
    "        else:\n",
    "            ham_posterior *= 1 / (len(ham_like_dict) + len(spam_like_dict))\n",
    "            \n",
    "        if word in spam_like_dict:\n",
    "            spam_posterior *= spam_like_dict[word]\n",
    "        else:\n",
    "            spam_posterior *= 1 / (len(ham_like_dict) + len(spam_like_dict))\n",
    "    \n",
    "    if ham_posterior >= spam_posterior:\n",
    "        ham_spam_decision = 0\n",
    "    else:\n",
    "        ham_spam_decision = 1\n",
    "\n",
    "    return ham_spam_decision\n",
    "    '''END'''\n",
    "\n",
    "def metrics(ham_prior, spam_prior, ham_dict, spam_dict, df):\n",
    "    '''\n",
    "    Calls \"predict\" function and report accuracy, precision, and recall of your prediction\n",
    "    '''\n",
    "    \n",
    "    '''YOUR CODE HERE'''\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        prediction = predict(ham_prior, spam_prior, ham_dict, spam_dict, row['text'])\n",
    "        \n",
    "        if row['label'] == 'spam' and prediction == 1:\n",
    "            true_positives += 1\n",
    "        elif row['label'] == 'ham' and prediction == 0:\n",
    "            true_negatives += 1\n",
    "        elif row['label'] == 'ham' and prediction == 1:\n",
    "            false_positives += 1\n",
    "        else:\n",
    "            false_negatives += 1\n",
    "            \n",
    "    acc = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "    if true_positives + false_positives == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    if true_positives + false_positives == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    return acc, precision, recall\n",
    "    '''END'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers with your functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2398 entries, 0 to 2397\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  2398 non-null   int64 \n",
      " 1   Unnamed: 0    2398 non-null   int64 \n",
      " 2   label         2398 non-null   object\n",
      " 3   text          2398 non-null   object\n",
      " 4   label_num     2398 non-null   int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 93.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#loading in the training data\n",
    "train_df = pd.read_csv(\"./TRAIN_balanced_ham_spam.csv\")\n",
    "test_df = pd.read_csv(\"./TEST_balanced_ham_spam.csv\")\n",
    "df = train_df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "#compute the prior\n",
    "\n",
    "ham_prior, spam_prior = prior(df)\n",
    "print(ham_prior, spam_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute likelihood\n",
    "ham_like_dict, spam_like_dict = likelihood(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Test your predict function with some example TEXT\n",
    "\n",
    "some_text_example = \"hello my name is tim\"\n",
    "print(predict(ham_prior, spam_prior, ham_like_dict, spam_like_dict, some_text_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334 0.9930555555555556 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Predict on test_df and compute metrics \n",
    "df = test_df\n",
    "ham_prior, spam_prior = prior(df)\n",
    "ham_dict, spam_dict = likelihood(df)\n",
    "acc, precision, recall = metrics(ham_prior, spam_prior, ham_dict, spam_dict, df)\n",
    "print(acc, precision, recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
